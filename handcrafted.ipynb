{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.disabled = True\n",
    "\n",
    "from agent import HandcraftedAgent\n",
    "\n",
    "env_str = \"room_env:RoomEnv-v2\"\n",
    "env_config = {\n",
    "    \"question_prob\": 1.0,\n",
    "    \"terminates_at\": 99,\n",
    "    \"randomize_observations\": \"objects\",\n",
    "    \"room_size\": \"l\",\n",
    "    \"rewards\": {\"correct\": 1, \"wrong\": 0, \"partial\": 0},\n",
    "    \"make_everything_static\": False,\n",
    "    \"num_total_questions\": 1000,\n",
    "    \"question_interval\": 1,\n",
    "    \"include_walls_in_observations\": True,\n",
    "    \"deterministic_objects\": False,\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for mm_policy in [\"random\", \"episodic\", \"semantic\", \"generalize\"]:\n",
    "    for qa_function in [\"episodic_semantic\", \"episodic\", \"semantic\", \"random\"]:\n",
    "        for explore_policy in [\"random\", \"avoid_walls\"]:\n",
    "            for capacity in [96]:\n",
    "                for pretrain_semantic in [False, \"include_walls\", \"exclude_walls\"]:\n",
    "                    for capacity_ in [\n",
    "                        {\"episodic\": capacity, \"semantic\": 0, \"short\": 1},\n",
    "                        {\"episodic\": 0, \"semantic\": capacity, \"short\": 1},\n",
    "                        {\n",
    "                            \"episodic\": capacity // 2,\n",
    "                            \"semantic\": capacity // 2,\n",
    "                            \"short\": 1,\n",
    "                        },\n",
    "                    ]:\n",
    "\n",
    "                        try:\n",
    "                            for seed in range(5):\n",
    "                                agent = HandcraftedAgent(\n",
    "                                    env_str=env_str,\n",
    "                                    env_config={**env_config, \"seed\": seed},\n",
    "                                    mm_policy=mm_policy,\n",
    "                                    qa_function=qa_function,\n",
    "                                    explore_policy=explore_policy,\n",
    "                                    capacity=capacity_,\n",
    "                                    pretrain_semantic=pretrain_semantic,\n",
    "                                    default_root_dir=f\"./training-results/handcrafted/room_size={env_config['room_size']}/mm_policy={mm_policy}/qa_function={qa_function}/explore_policy={explore_policy}/episodiccapacity={capacity_['episodic']}/semanticcapacity={capacity_['semantic']}/shortcapacity={capacity_['short']}/pretrain_semantic={pretrain_semantic}\",\n",
    "                                )\n",
    "                                agent.test()\n",
    "                        except Exception as e:\n",
    "                            print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before grouping: 5760 results\n",
      "After grouping: 1152 results\n",
      "After simplifying: 1152 results\n",
      "After filtering: 204 results\n",
      "DataFrame saved to 'hand-crafted-results-room_size=xl.csv'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "from humemai.utils import read_yaml\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def parse_hyper_params_from_path(path):\n",
    "    \"\"\"Extracts hyperparameters from the given file path.\"\"\"\n",
    "    try:\n",
    "        path_parts = path.split(\"/\")\n",
    "        return {\n",
    "            \"mm_policy\": path_parts[4].split(\"=\")[-1],\n",
    "            \"qa_function\": path_parts[5].split(\"=\")[-1],\n",
    "            \"explore_policy\": path_parts[6].split(\"=\")[-1],\n",
    "            \"episodic_capacity\": int(path_parts[7].split(\"=\")[-1]),\n",
    "            \"semantic_capacity\": int(path_parts[8].split(\"=\")[-1]),\n",
    "            \"long_capacity\": int(path_parts[7].split(\"=\")[-1])\n",
    "            + int(path_parts[8].split(\"=\")[-1]),\n",
    "            \"short_capacity\": int(path_parts[9].split(\"=\")[-1]),\n",
    "            \"pretrain_semantic\": path_parts[10].split(\"=\")[-1],\n",
    "        }\n",
    "    except (IndexError, ValueError) as e:\n",
    "        print(f\"Error parsing hyperparameters from path {path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def load_results(yaml_paths):\n",
    "    \"\"\"Loads YAML data from a list of file paths.\"\"\"\n",
    "    results = []\n",
    "    for path in yaml_paths:\n",
    "        try:\n",
    "            results.append(read_yaml(path))\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading YAML file {path}: {e}\")\n",
    "    return results\n",
    "\n",
    "\n",
    "room_size = \"xl\"\n",
    "results_paths = glob(\n",
    "    f\"./training-results/handcrafted/room_size={room_size}/*/*/*/*/*/*/*/*/results.yaml\"\n",
    ")\n",
    "\n",
    "print(f\"Before grouping: {len(results_paths)} results\")\n",
    "\n",
    "# Group results by hyperparameters\n",
    "grouped_results = defaultdict(list)\n",
    "for path in results_paths:\n",
    "    hyper_params = parse_hyper_params_from_path(path)\n",
    "    if hyper_params is not None:\n",
    "        results_data = read_yaml(path)\n",
    "        hp_tuple = tuple(sorted(hyper_params.items()))\n",
    "        grouped_results[hp_tuple].append(results_data)\n",
    "\n",
    "print(f\"After grouping: {len(grouped_results)} results\")\n",
    "\n",
    "# Simplify results to mean and std of test scores\n",
    "simplified_results = []\n",
    "for hp_tuple, results in grouped_results.items():\n",
    "    mean_scores = [result[\"test_score\"][\"mean\"] for result in results]\n",
    "\n",
    "    mean_of_means = np.mean(mean_scores)\n",
    "    std_of_means = np.std(mean_scores)\n",
    "\n",
    "    simplified_results.append(\n",
    "        {\n",
    "            \"hyper_params\": dict(hp_tuple),\n",
    "            \"results\": {\n",
    "                \"test_mean\": mean_of_means.item(),\n",
    "                \"test_std\": std_of_means.item(),\n",
    "            },\n",
    "        }\n",
    "    )\n",
    "print(f\"After simplifying: {len(simplified_results)} results\")\n",
    "\n",
    "# filtered results\n",
    "filtered_results = []\n",
    "for result in simplified_results:\n",
    "    if result[\"hyper_params\"][\"episodic_capacity\"] == 0:\n",
    "        if result[\"hyper_params\"][\"mm_policy\"] == \"episodic\":\n",
    "            continue\n",
    "        if result[\"hyper_params\"][\"qa_function\"] == \"episodic\":\n",
    "            continue\n",
    "\n",
    "    if result[\"hyper_params\"][\"semantic_capacity\"] == 0:\n",
    "        if result[\"hyper_params\"][\"mm_policy\"] == \"semantic\":\n",
    "            continue\n",
    "        if result[\"hyper_params\"][\"qa_function\"] == \"semantic\":\n",
    "            continue\n",
    "\n",
    "    if result[\"hyper_params\"][\"mm_policy\"] == \"episodic\":\n",
    "        if result[\"hyper_params\"][\"qa_function\"] == \"semantic\":\n",
    "            continue\n",
    "        if result[\"hyper_params\"][\"qa_function\"] == \"random\":\n",
    "            continue\n",
    "        if result[\"hyper_params\"][\"qa_function\"] == \"episodic_semantic\":\n",
    "            continue\n",
    "        if result[\"hyper_params\"][\"semantic_capacity\"] > 0:\n",
    "            continue\n",
    "\n",
    "    if result[\"hyper_params\"][\"mm_policy\"] == \"semantic\":\n",
    "        if result[\"hyper_params\"][\"qa_function\"] == \"episodic\":\n",
    "            continue\n",
    "        if result[\"hyper_params\"][\"qa_function\"] == \"random\":\n",
    "            continue\n",
    "        if result[\"hyper_params\"][\"qa_function\"] == \"episodic_semantic\":\n",
    "            continue\n",
    "        if result[\"hyper_params\"][\"episodic_capacity\"] > 0:\n",
    "            continue\n",
    "\n",
    "    if result[\"hyper_params\"][\"mm_policy\"] == \"generalize\":\n",
    "        if result[\"hyper_params\"][\"qa_function\"] == \"episodic\":\n",
    "            continue\n",
    "        if result[\"hyper_params\"][\"qa_function\"] == \"semantic\":\n",
    "            continue\n",
    "\n",
    "    if result[\"hyper_params\"][\"mm_policy\"] == \"random\":\n",
    "        if result[\"hyper_params\"][\"qa_function\"] == \"episodic\":\n",
    "            continue\n",
    "        if result[\"hyper_params\"][\"qa_function\"] == \"semantic\":\n",
    "            continue\n",
    "\n",
    "    if result[\"hyper_params\"][\"pretrain_semantic\"] == \"exclude_walls\":\n",
    "        continue\n",
    "\n",
    "    filtered_results.append(result)\n",
    "\n",
    "print(f\"After filtering: {len(filtered_results)} results\")\n",
    "\n",
    "\n",
    "# Create a DataFrame from the combined results\n",
    "data = []\n",
    "for item in filtered_results:\n",
    "    row = item[\"hyper_params\"]\n",
    "    row.update(item[\"results\"])\n",
    "    data.append(row)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Define the desired column order\n",
    "column_order = [\n",
    "    \"mm_policy\",\n",
    "    \"qa_function\",\n",
    "    \"explore_policy\",\n",
    "    \"pretrain_semantic\",\n",
    "    \"long_capacity\",\n",
    "    \"episodic_capacity\",\n",
    "    \"semantic_capacity\",\n",
    "    \"short_capacity\",\n",
    "    \"test_mean\",\n",
    "    \"test_std\",\n",
    "]\n",
    "\n",
    "\n",
    "df = df.sort_values(by=column_order, ascending=True)\n",
    "df = df.sort_values(by=[\"long_capacity\", \"test_mean\"], ascending=[True, False])\n",
    "\n",
    "# Reorder the DataFrame columns\n",
    "df = df[column_order]\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv(f\"hand-crafted-results-room_size={room_size}.csv\", index=False)\n",
    "\n",
    "# Confirm the DataFrame is saved by printing the location\n",
    "print(f\"DataFrame saved to 'hand-crafted-results-room_size={room_size}.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "human-memory",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
